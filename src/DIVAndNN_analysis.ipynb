{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align: center;\">BlueCloud Zooplankton Demonstrator</h1>\n",
    "\n",
    "<p style=\"text-align: center;\">Alexander Barth, Charles Troupin</p>\n",
    "<p style=\"text-align: center;\">GHER, University of Li√®ge, Belgium</p>\n",
    "\n",
    "The aim of this notebook is to created a gridded dataset of the\n",
    "[Continuous Plankton Recorder](https://www.cprsurvey.org/services/the-continuous-plankton-recorder/) within the Virtual Research Environement developped in BlueCloud.\n",
    "\n",
    "\n",
    "### Method\n",
    "\n",
    "DIVA (Data Interpolating Variational Analysis) aim to derive a gridded climatology from in situ observations. The  derived field should be\n",
    " __close to the observations__ (it should not necessarily pass through all observations because observations have errors), __close to a first guess estimate__, \"__smooth__\" (i.e. small first and second ordrer derivatives). This is formalized using the following cost functions.\n",
    "\n",
    "$\\newcommand{\\vec}{\\mathbf}\\newcommand{\\mat}{\\mathbf}$\n",
    "$$\n",
    "J(\\vec x) =\n",
    "\\left(\\mat H \\vec x - \\vec y \\right)^T \\mat R^{-1}  \\left(\\mat H \\vec x - \\vec y \\right)\n",
    "+\n",
    "\\left(\\vec x - \\vec x^b \\right)^T \\mat B^{-1}  \\left(\\vec x - \\vec x^b \\right)\n",
    "$$\n",
    "\n",
    "DIVAnd typically use obervations from a single parameter to create gridded datasets.\n",
    "In this notebook we explore the use of multivariate analysis using neural networks and DIVAnd.\n",
    "Assuming that there is a list of covariables $\\vec z_1, \\vec z_2, ...$ related to the parameter of interest $\\vec x$, we assume that gridded field can be writtes as:\n",
    "\n",
    "$$\n",
    "\\vec x = \\vec x' + f(\\vec z_1,\\vec z_2,..., \\mat W_1, \\vec b_1, \\mat W_2, \\vec b_2,...)\n",
    "$$\n",
    "\n",
    "where $f$ is a non-linear function of the known covariables and unknown parameters $\\mat W_1, \\vec b_1, \\mat W_2, \\vec b_2,...$)\n",
    "The structure of the function $f$ is given here by a neural network (multilayer perception).\n",
    "The field $\\vec x'$ is also unknown. It is subjected to the smoothness contraints from DIVAnd.\n",
    "\n",
    "We want to minimize:\n",
    "\n",
    "$$\\begin{eqnarray}\n",
    "J(\\vec x',\\mat W_1, \\vec b_1, \\mat W_2, \\vec b_2,...) &=&\n",
    "\\left(\\mat H \\vec x - \\vec y \\right)^T \\mat R^{-1}  \\left(\\mat H \\vec x - \\vec y \\right) \\\\\\\\\n",
    "&& +\n",
    "\\left(\\vec x' - \\vec x^b \\right)^T \\mat B^{-1}  \\left(\\vec x' - \\vec x^b \\right)\n",
    "\\end{eqnarray}$$\n",
    "\n",
    "\n",
    "For every location $j$, the value of vector $\\vec v^1$ are the co-variables at the location $j$.\n",
    "This vector is linearly transformed by a matrix $\\mat W_k$ and a vector $\\vec b_k$ and then a non-linear activation function is applied to each element element of the resulting vector (except for the last step).\n",
    "\n",
    "$$\n",
    "\\mat v^{(k+1)}_j = g^{(k+1)}(\\mat v^{(k)}_j \\mat W_k + \\vec b_k)\n",
    "$$\n",
    "\n",
    "Here, the weight do not dependent on space but the longitude and latitude are one of the covariables.\n",
    "\n",
    "The following \"Co-variables\" are used (with the data source in parenthesis):\n",
    "* Sea water temperature (SeaDataCloud)\n",
    "* Salinity (SeaDataCloud)\n",
    "* Distance from coast (NASA Goddard Space Flight Center)\n",
    "* Bathymetry (GEBCO)\n",
    "* Nitrate, Silicate and Phosphate (World Ocean Atlas 2018)\n",
    "\n",
    "\n",
    "The following figure gives a high-level overview of the different datasets involed in this notebook.\n",
    "\n",
    "![overview](docs/overview.svg)\n",
    "\n",
    "\n",
    "\n",
    "_This notebook is release under the terms of the GPL version 2 (or later, at your option)_\n",
    "\n",
    "The first step is to install all dependencies (if necessary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "\n",
    "try\n",
    "    # check if DIVAndNN is already installed\n",
    "    using DIVAndNN\n",
    "catch\n",
    "    # install all dependencies\n",
    "    pkg\"add https://github.com/gher-ulg/DIVAndNN.jl\"\n",
    "    pkg\"add JSON PyCall PyPlot DIVAnd Glob DataStructures NCDatasets\"\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "srcdir = @__DIR__\n",
    "if !isfile(joinpath(srcdir,\"grid.jl\"))\n",
    "    srcdir = get(ENV,\"SRCDIR\",\"/workspace/VREFolders/Zoo-Phytoplankton_EOV/Zooplankton_EOV/bluecloud-plankton-master/src/\")\n",
    "end\n",
    "using DIVAnd\n",
    "using DIVAndNN\n",
    "using Random\n",
    "using NCDatasets\n",
    "using DelimitedFiles\n",
    "using Statistics\n",
    "push!(LOAD_PATH,srcdir)\n",
    "push!(LOAD_PATH,@__DIR__)\n",
    "using BlueCloudPlankton\n",
    "using DataStructures\n",
    "using Printf\n",
    "using Dates\n",
    "using JSON\n",
    "using PyPlot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set seed for random number generator and include grid information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(1234)\n",
    "include(joinpath(srcdir,\"grid.jl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data frome the continuous plankton recorder\n",
    "https://www.cprsurvey.org/services/the-continuous-plankton-recorder/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "datafile = joinpath(datadir, \"data-cpr.csv\")\n",
    "maybedownload(\"https://dox.ulg.ac.be/index.php/s/ME3U5wPdPK8GRFu/download\",\n",
    "              datafile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the depth range of the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, columnnames = readdlm(datafile, ',', header=true);\n",
    "getcolumn(name) = data[:,findfirst(columnnames[:] .== name)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the depth range of the data\n",
    "minimumDepthInMeters = getcolumn(\"minimumDepthInMeters\")\n",
    "maximumDepthInMeters = getcolumn(\"maximumDepthInMeters\")\n",
    "@info \"range of minimum depth $(extrema(minimumDepthInMeters))\"\n",
    "@info \"range of maximum depth $(extrema(maximumDepthInMeters))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prepare following fields:\n",
    "* [Temperature and salinity from SeaDataCloud](https://doi.org/10.13155/77512)\n",
    "* Nitrate, silicate and phosphate from [World Ocean Atlas 2018](https://www.ncei.noaa.gov/products/world-ocean-atlas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_TS = [\n",
    "    (\"http://www.ifremer.fr/erddap/griddap/SDC_GLO_CLIM_TS_V2_1\",\"Salinity\",\"salinity\"),\n",
    "    (\"http://www.ifremer.fr/erddap/griddap/SDC_GLO_CLIM_TS_V2_1\",\"Temperature\",\"temperature\"),\n",
    "    (\"https://www.ncei.noaa.gov/thredds-ocean/dodsC/ncei/woa/nitrate/all/1.00/woa18_all_n00_01.nc\",\"n_an\",\"nitrate\"),\n",
    "    (\"https://www.ncei.noaa.gov/thredds-ocean/dodsC/ncei/woa/silicate/all/1.00/woa18_all_i00_01.nc\",\"i_an\",\"silicate\"),\n",
    "    (\"https://www.ncei.noaa.gov/thredds-ocean/dodsC/ncei/woa/phosphate/all/1.00/woa18_all_p00_01.nc\",\"p_an\",\"phosphate\"),\n",
    "]\n",
    "\n",
    "maybedownload(\"https://dox.ulg.ac.be/index.php/s/7zwCEszAPIFeBAm/download\",joinpath(datadir,\"salinity.nc\"))\n",
    "maybedownload(\"https://dox.ulg.ac.be/index.php/s/OQMYYGFCEtS3xc9/download\",joinpath(datadir,\"temperature.nc\"))\n",
    "\n",
    "DIVAndNN.prep_tempsalt(gridlon,gridlat,data_TS,datadir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take all years"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "years = 0:3000\n",
    "ndimensions = 2\n",
    "\n",
    "#years = 1990:2017\n",
    "#ndimensions = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get bathymetry from GEBCO and prepare land-sea mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "bathname = joinpath(datadir,\"gebco_30sec_4.nc\");\n",
    "bathisglobal = true;\n",
    "maybedownload(\"https://dox.ulg.ac.be/index.php/s/RSwm4HPHImdZoQP/download\",\n",
    "              joinpath(datadir,\"gebco_30sec_4.nc\"))\n",
    "\n",
    "maskname = joinpath(datadir,\"mask.nc\");\n",
    "\n",
    "if !isfile(maskname)\n",
    "    DIVAndNN.prep_mask(bathname,bathisglobal,gridlon,gridlat,years,maskname)\n",
    "end\n",
    "\n",
    "ds = Dataset(maskname,\"r\")\n",
    "mask = nomissing(ds[\"mask\"][:,:]) .== 1\n",
    "close(ds)\n",
    "\n",
    "DIVAndNN.prep_bath(bathname,bathisglobal,gridlon,gridlat,datadir)\n",
    "\n",
    "if ndimensions == 3\n",
    "    mask = repeat(mask,inner=(1,1,length(years)))\n",
    "end\n",
    "\n",
    "if ndimensions == 3\n",
    "    mask2,pmn,xyi = DIVAnd.domain(bathname,bathisglobal,gridlon,gridlat,years);\n",
    "else\n",
    "    mask2,pmn,xyi = DIVAnd.domain(bathname,bathisglobal,gridlon,gridlat);\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distance to coast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interp_fname = joinpath(datadir,\"dist2coast_subset.nc\")\n",
    "fname_dist2coast = \"https://pae-paha.pacioos.hawaii.edu/thredds/dodsC/dist2coast_1deg\"\n",
    "if !isfile(interp_fname)\n",
    "    DIVAndNN.prep_dist2coast(fname_dist2coast,gridlon,gridlat,interp_fname)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "load covariables for the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "covars_coord = false\n",
    "covars_const = true\n",
    "\n",
    "covars_fname = [\n",
    "    (\"bathymetry.nc\",       \"batymetry\",  identity),\n",
    "    (\"dist2coast_subset.nc\",\"distance\",   identity),\n",
    "    #(\"salinity.nc\",\"salinity\",log),\n",
    "    (\"salinity.nc\",         \"salinity\",   identity),\n",
    "    (\"temperature.nc\",      \"temperature\",identity),\n",
    "    (\"nitrate.nc\",          \"nitrate\",    identity),\n",
    "    (\"phosphate.nc\",        \"phosphate\",  identity),\n",
    "    (\"silicate.nc\",         \"silicate\",   identity),\n",
    "]\n",
    "\n",
    "covars_fname = map(entry -> (joinpath(datadir,entry[1]),entry[2:end]...),covars_fname)\n",
    "\n",
    "field = DIVAndNN.loadcovar((gridlon,gridlat),covars_fname;\n",
    "                           covars_coord = covars_coord,\n",
    "                           covars_const = covars_const)\n",
    "\n",
    "if ndimensions == 3\n",
    "    sz = size(field)\n",
    "    field = repeat(reshape(field,(sz[1],sz[2],1,sz[3])),1,1,length(years),1)\n",
    "end\n",
    "DIVAndNN.normalize!(mask,field)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon, lat, dates, abundance, scientificNames = BlueCloudPlankton.read_data(datafile);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all unique scientific names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "scientificname_accepted = unique(scientificNames)\n",
    "println.(unique(scientificNames));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly choose cross-validation point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvfname = replace(datafile,\".csv\" => \"-cv.nc\")\n",
    "validation_fraction = 0.2\n",
    "\n",
    "if isfile(cvfname)\n",
    "    @info \"load $cvfname\"\n",
    "\n",
    "    for_cv =\n",
    "        Dataset(cvfname,\"r\") do ds\n",
    "            Bool.(ds[\"for_cv\"][:])\n",
    "        end\n",
    "else\n",
    "    @info \"split data\"\n",
    "    for_cv = rand(length(lon)) .< validation_fraction\n",
    "\n",
    "    Dataset(cvfname,\"c\") do ds\n",
    "        defVar(ds,\"for_cv\",Int8.(for_cv),(\"observation\",),attrib = OrderedDict(\n",
    "            \"long_name\" => \"0: observation used for analysis; 1 observation used for validation\")\n",
    "               )\n",
    "    end\n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of data used for cross-validation and for the analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@info \"Number of cross-valiation points: $(sum(for_cv))\"\n",
    "@info \"Number of data points for the analysis: $(sum(.!for_cv))\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "time correlation (for 3d analyses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lent = 0. # years\n",
    "if ndimensions == 3\n",
    "    lent = 3.\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "plotevery = 100\n",
    "niter = 500\n",
    "trainfrac = 1.\n",
    "\n",
    "epsilon2ap = 10\n",
    "epsilon2_background = 10\n",
    "epsilon2_cpme = epsilon2_background\n",
    "\n",
    "NLayers = [size(field)[end],4,1]\n",
    "\n",
    "learning_rate = 0.001\n",
    "L2reg = 0.0001\n",
    "dropoutprob = 0.6\n",
    "\n",
    "len = 300e3;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = joinpath(resdir,\"results-ncovars$(length(covars_fname))-epsilon2ap$(epsilon2ap)-len$(len)-niter$(niter)-nlayers$(length(NLayers))-ndimensions$(ndimensions)\")\n",
    "mkpath(outdir)\n",
    "\n",
    "nameindex = 1\n",
    "for nameindex in 1:length(scientificname_accepted)\n",
    "\n",
    "    sname = String(scientificname_accepted[nameindex])\n",
    "    global loss_iter\n",
    "    global val_iter\n",
    "    @info sname\n",
    "\n",
    "    paramname = joinpath(outdir,\"DIVAndNN_$(sname)_interp.json\")\n",
    "\n",
    "    if isfile(paramname)\n",
    "        #                continue\n",
    "    end\n",
    "\n",
    "    s = (sname .== scientificNames) .& .!for_cv\n",
    "    lon_a,lat_a,obstime_a,value_a = lon[s], lat[s], dates[s], abundance[s]\n",
    "    @info \"number of observation used in analysis: $(sum(s))\"\n",
    "\n",
    "    s = (sname .== scientificNames) .& for_cv\n",
    "    lon_cv,lat_cv,obstime_cv,value_cv = lon[s], lat[s], dates[s], abundance[s]\n",
    "    @info \"number of observation used for validation: $(sum(s))\"\n",
    "\n",
    "    time_a = Float64.(Dates.year.(obstime_a))\n",
    "    time_cv = Float64.(Dates.year.(obstime_cv))\n",
    "\n",
    "    @debug begin\n",
    "        @show value_a[1:min(end,10)]\n",
    "        @show extrema(value_a)\n",
    "        @show length(value_a)\n",
    "    end\n",
    "\n",
    "    Random.seed!(1234)\n",
    "\n",
    "    value_analysis = zeros(size(mask))\n",
    "\n",
    "    if ndimensions == 3\n",
    "        xobs_a = (lon_a,lat_a,time_a)\n",
    "        xobs_cv = (lon_cv,lat_cv,time_cv)\n",
    "        lenxy = (len,len,lent)\n",
    "        analysis_grid = (gridlon,gridlat,years)\n",
    "        analysis_grid2 = (gridlon,gridlat,DateTime.(years,1,1))\n",
    "    else\n",
    "        xobs_a = (lon_a,lat_a)\n",
    "        xobs_cv = (lon_cv,lat_cv)\n",
    "        lenxy = (len,len)\n",
    "        analysis_grid = (gridlon,gridlat)\n",
    "        analysis_grid2 = (gridlon,gridlat)\n",
    "    end\n",
    "\n",
    "\n",
    "    value_analysis,fw0 = DIVAndNN.analysisprob(\n",
    "        mask,pmn,xyi,xobs_a,\n",
    "        value_a,\n",
    "        lenxy,epsilon2ap,\n",
    "        field,\n",
    "        NLayers,\n",
    "        costfun = DIVAndNN.regression,\n",
    "        niter = niter,\n",
    "        dropoutprob = dropoutprob,\n",
    "        L2reg = L2reg,\n",
    "        learning_rate = learning_rate,\n",
    "        rmaverage = true,\n",
    "        trainfrac = trainfrac,\n",
    "        epsilon2_background = epsilon2_background,\n",
    "    )\n",
    "\n",
    "    vp = DIVAndNN.validate_regression(analysis_grid,value_analysis,xobs_cv,value_cv)\n",
    "    outname = joinpath(outdir,\"DIVAndNN_$(sname)_interp.nc\")\n",
    "\n",
    "    cpme = DIVAnd_cpme(mask,pmn,xyi,xobs_a,value_a,lenxy,epsilon2_cpme)\n",
    "    DIVAnd.save(outname,analysis_grid2,value_analysis,sname; relerr = cpme)\n",
    "\n",
    "    open(paramname,\"w\") do f\n",
    "        write(f,JSON.json(\n",
    "            Dict(\n",
    "                \"validation\" => vp,\n",
    "                \"L2reg\" =>            L2reg,\n",
    "                \"dropoutprob\" =>      dropoutprob,\n",
    "                \"epsilon2ap\" =>       epsilon2ap,\n",
    "                \"epsilon2_background\" => epsilon2_background,\n",
    "                \"len\" =>              len,\n",
    "                \"niter\" =>            niter,\n",
    "                \"learning_rate\" =>    learning_rate,\n",
    "                \"NLayers\" =>    NLayers,\n",
    "                \"name\" =>    sname,\n",
    "                \"covars\" => first.(covars_fname),\n",
    "            )\n",
    "        ))\n",
    "    end\n",
    "\n",
    "end\n",
    "\n",
    "score = DIVAndNN.summary(outdir)\n",
    "\n",
    "paramname2 = joinpath(outdir,\"DIVAndNN.json\")\n",
    "\n",
    "open(paramname2,\"w\") do f\n",
    "    write(f,JSON.json(\n",
    "        Dict(\n",
    "            \"validation\" => score,\n",
    "            \"L2reg\" =>            L2reg,\n",
    "            \"dropoutprob\" =>      dropoutprob,\n",
    "            \"epsilon2ap\" =>       epsilon2ap,\n",
    "            \"epsilon2_background\" => epsilon2_background,\n",
    "            \"len\" =>              len,\n",
    "            \"niter\" =>            niter,\n",
    "            \"learning_rate\" =>    learning_rate,\n",
    "            \"NLayers\" =>    NLayers,\n",
    "            \"covars\" => first.(covars_fname),\n",
    "        )\n",
    "    ))\n",
    "end;\n",
    "\n",
    "\n",
    "@info \"Results have been saved in $(outdir). Consider to copy the files to a permanent storage.\""
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "formats": "ipynb,jl:percent"
  },
  "kernelspec": {
   "display_name": "Julia 1.7.0",
   "language": "julia",
   "name": "julia-1.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
